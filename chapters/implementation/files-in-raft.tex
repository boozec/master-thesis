\section{Files in a Raft Cluster}

As discussed in Section \ref{sec:raft-cluster-for-file-uploads}, the system is built upon a Raft cluster integrated with a storage mechanism based on Reed-Solomon coding. To study this integration, a Go\footnote{\url{https://go.dev/}} application was implemented, reconstructing a Cubbit-like infrastructure by layering a Raft cluster on top of Reed-Solomon redundancy. Go was chosen for its gentle learning curve and widespread adoption, including by companies such as Cubbit. Following the common project layout recommended for Go\cite{go-modules-layout} all the developed services are organized within a single module. This structure avoids redundancy, such as duplicating the service message definitions described later. Each service is maintained separately as a distinct binary under the \texttt{cmd} folder, in accordance with Go best practices.



To investigate the communication patterns between the system components, two service
APIs were implemented: a REST API, commonly used for web services, and a
gRPC\footnote{\url{https://grpc.io/}}, developed by Google and based on Protocol
Buffers (Protobuf), which allows precise definition of the message types transmitted over the network.

Listing \ref{code:protobuf-for-agent} illustrates the Protobuf definitions used by the agents, including messages for both gateway-agent and agent-agent interactions.

The distinctive feature of gRPC is that the Protobuf file can be directly compiled into Go code using a simple CLI, as shown in Listing \ref{code:protobuf-protoc}.

\begin{listing}[H]
\caption{Protobuf compiler command that generates Go code from the service definition located at \texttt{agent.proto}.}
\label{code:protobuf-protoc}
\begin{minted}{shell}
protoc --go_out=. \
        --go_opt=paths=source_relative \
        --go-grpc_out=. \
        --go-grpc_opt=paths=source_relative \
        agent.proto
\end{minted}
\end{listing}

This command generates two Go files: 
\begin{itemize}
    \item \texttt{agent.pb.go}, which contains the standard Protobuf message definitions and serialization code.
    \item \texttt{agent\_grpc.pb.go}, which contains the gRPC client and server stubs necessary to implement the service endpoints in Go. The methods defined in Listing \ref{code:protobuf-for-agent} are represented as an interface in this file, and they must be implemented to enable communication between a gateway and the agent service, or among agents themselves.
\end{itemize}

Together, these files constitute the foundation for both gateway-to-agent and inter-agent communication in the system.

\subsection{Gateway service} 

The Gateway service acts as the entry point for the user, as illustrated in step 1 of Figure \ref{fig:sequence-diagram-upload-file}. It is exposed via a REST API and manages file uploads and downloads by splitting files into $n+k$ shards using the Reed-Solomon algorithm and distributing them across the agents. The REST interface is intentionally minimal in this prototype, offering only two endpoints: one for uploading and one for downloading files. The upload endpoint requires the local file path and the desired filename, while the download endpoint requires only the filename.

Go provides a straightforward mechanism to instantiate a web server, as shown in Listing \ref{code:mux-gateway}. The \texttt{mux} server maps REST paths to functions, referred to as handlers. 

\begin{listing}\caption{Instantiation of a simple web server in Go.}

\label{code:mux-gateway}
\begin{minted}[linenos,fontsize=\footnotesize]{go}
mux := &http.ServeMux{}
mux.HandleFunc("/upload", UploadHandler)
mux.HandleFunc("/download/{filename}", DownloadHandler)
http.ListenAndServe("<url>", mux);
\end{minted}
\end{listing}


\paragraph{Upload handler}

A partial implementation of the upload handler is shown in Listing \ref{code:upload-handler}.

Some clarifications on the code in Listing \ref{code:upload-handler} are in order.
The details of file encryption are omitted, since they are not central to the current discussion. Similarly, the internal implementation of Reed-Solomon encoding is abstracted: the key point is that the function \texttt{reedSolomon.Create} returns the $n+k$ shards from the encrypted file. Error handling is also simplified for readability, and the specific contents of the \texttt{jsonResponse} are not reported, as they are not relevant to the design.

The critical part lies in lines 14-16, where the \texttt{SendShard} method from the custom \texttt{rpc} module is invoked. This method serves as a wrapper around the generated gRPC client code contained in \texttt{agent\_grpc.pb.go}. Its role is to establish the connection, transmit the shard, and delegate the call to the gRPC stub. The implementation of this wrapper is presented in Listing \ref{code:rpc-send-shard}.

However, this implementation alone is not sufficient. A corresponding gRPC server must also be defined to handle the data transmitted in lines 27-31. As explained earlier, the code in \texttt{agent\_grpc.pb.go} only provides the interface definition; the concrete server-side logic needs to be implemented in order to process the incoming shard data. This aspect will be discussed in detail in the next section.

\paragraph{Download handler}

Equally important, though similar in structure to the previously presented \texttt{UploadHandler}, is the \texttt{DownloadHandler}. While not strictly necessary for system testing, it is useful in both real-world scenarios and tests, for example to verify that a downloaded file is not corrupted. A partial implementation is shown in Listing \ref{code:download-handler}.

As with the \texttt{UploadHandler}, certain implementation details are omitted for simplicity, such as how the correct salt is determined for each file and the internal workings of the Reed-Solomon reconstruction algorithm. The \texttt{rpc.GetAllShards} function serves as a wrapper, analogous to \texttt{SendShard} in Listing \ref{code:rpc-send-shard}. Internally, it executes gRPC calls to the \texttt{GetShard} method defined in the Protobuf file (Listing \ref{code:protobuf-for-agent}). The server-side handling will be discussed in the following section.

\newpage

\subsection{Agent service}

Each agent is responsible for storing its assigned shards locally and participates as a node in the Raft cluster. The total number of agents corresponds to the Reed-Solomon configuration $n+k$. Communication between the gateway and agents is implemented using gRPC, which enables efficient binary data transfer and supports a wide range of commands. Inter-agent communication also relies on gRPC, providing operations such as shard acknowledgments, cluster membership management, and retrieval of the current Merkle root hash for a given folder.

\paragraph{Instance new agent}

The Agent service must instantiate a new gRPC server at startup, while also initializing its participation in the Raft cluster. When a node starts a new cluster, the process is referred to as \emph{bootstrapping}. A simplified version of the Agent service startup is shown in Listing \ref{code:agent-main-go}.

Some details are omitted in Listing \ref{code:agent-main-go}, such as error handling, but two elements are particularly important:
the \texttt{Server} struct, which implements the service interface generated from the Protobuf definition, and the \texttt{newRaft} function, which initializes a new Raft instance.  

For this prototype, the HashiCorp Raft implementation\footnote{\url{https://github.com/hashicorp/raft}} has been adopted. A partial implementation of \texttt{newRaft} is shown in Listing \ref{code:new-raft-function}.

Here, the \texttt{id} field identifies the node (e.g., \texttt{nodeA}), while \texttt{raftAddress} specifies the address where the Raft instance listens (e.g., \texttt{0.0.0.0:4001}). This address is distinct from the one used by the gRPC server.

Several components in Listing \ref{code:new-raft-function} deserve further attention.  
The \texttt{logs} store persists the sequence of Raft log entries, ensuring that all operations proposed to the cluster are durable. The \texttt{stable} store maintains critical metadata, such as the current term and cluster configuration, which must survive restarts. The \texttt{snaps} store provides periodic snapshots of the state machine, enabling log compaction and preventing unbounded growth. Finally, the \texttt{fsm} represents the finite state machine, which applies committed log entries to the storage layer, managing shard placement and retrieval.  

Together, these components enable each agent to participate reliably in the Raft consensus protocol, preserving consistency and fault tolerance across the distributed system.

The reader may notice that the cluster is bootstrapped only within the
\texttt{if} statement in lines 24-33 of Listing \ref{code:new-raft-function}.
The \texttt{bootstrap} flag is passed when calling \texttt{newRaft} at startup.
Importantly, only one node per cluster performs the bootstrap step during
initialization. All other agents must join the cluster by sending a \texttt{JoinRaft} request over gRPC, as illustrated in Listing \ref{code:grpc-call-join-request}.

Meanwhile, the bootstrap node must handle these incoming gRPC requests, which is possible because every Raft node is also an agent, as shown in Listing \ref{code:grpc-response-join-request}. As always, some details are omitted to have simpler snippets, such as the various error handling.

\paragraph{Handlers for upload and download}

The two gRPC calls \texttt{SendShard} and \texttt{GetShard}, introduced in Listings \ref{code:rpc-send-shard} and \ref{code:download-handler}, are handled by the agents' gRPC servers, as illustrated in Listings \ref{code:send-shard-server} and \ref{code:get-shard-server}. For simplicity, error handling and some path resolution details are omitted.

The key aspect to note is the inner go-function in Listing \ref{code:send-shard-server}, lines 9-28. This function retrieves the top-level and second-level folder roots for the shard. Differently than Figure \ref{fig:sequence-diagram-upload-file}, it then prepares a single gRPC request containing both roots (concatenated; each root is 32 bytes) and sends it to the leader. If the sender itself is the leader, the message is processed locally.

\paragraph{Command structure}

As introduced in Section \ref{sec:recovery-of-missing-shards}, agents send acknowledgments to confirm that a shard has been successfully stored during a file upload. This mechanism can also be observed in the \texttt{SendShard} function in Listing \ref{code:send-shard-server}, lines 24 and 26.

Before presenting the server-side implementation of the \texttt{AckShard} call,
it is necessary to clarify how messages are applied to the Raft log. This is
achieved through a command-style object, consisting of an operation code (a
simple integer) and an action value (an \texttt{interface\{\}} in Go). The Raft log interprets entries by first examining the operation code and then processing the corresponding action value. The definition is shown in Listing \ref{code:command-struct-for-raft}.


The action value can then be extracted with a cast to a concrete type and
dispatched using a \texttt{switch} statement on instance's \texttt{Code}.

\paragraph{Acknowledgment}

For simplicity, some details on the handling of the \texttt{AckShard} call on the server-side are omitted, including error handling and the explicit check that the current node processing the
 request is indeed the cluster leader. The server-side implementation is shown in Listing \ref{code:ack-shard}, while Listing \ref{code:raft-method-to-add-log} illustrates how new messages are
 applied to the Raft log.

The function \texttt{shardAcknoledge} updates the acknowledgment status for a
given shard using a boolean array, where the $i$-th position corresponds to
Agent $i$. Updates are applied with a bitwise-or (\texttt{done[i] || prevDone[i]}) and protected with a mutex to prevent race conditions during concurrent \texttt{r.Apply} calls.

The functions \texttt{saveRoot} and \texttt{saveAgentHash} persist Merkle roots and per-agent roots, respectively, as shown in Listing \ref{code:raft-method-to-add-log}. Unlike in previous sections, the two folder levels are split into four global maps (\texttt{roots1}, \texttt{roots2}, \texttt{hashes1}, \texttt{hashes2}), which simplifies extensions to deeper folder hierarchies while also improving efficiency.

Each command action is associated with a concrete type such as \texttt{ShardDone}, \texttt{MerkletreeRootHash}, or \texttt{MerkletreeHash}. This design choice makes it easier to parse and interpret actions later on. Since Raft log messages are stored as raw bytes, serializing them into structured types provides a reliable way to access and manipulate individual parameters.

Finally, the \texttt{merkletree.RootHash} function, used to compute folder roots, will be discussed in detail in the following paragraph.

\paragraph{Compute Merkle trees}

As seen in the previous paragraph, there is a call to \texttt{merkletree.RootHash}, which
takes as input an array of per-agent hashes of a top-level or second-level folder. 
Similarly, the reader may also have noticed a call to 
\texttt{merkletree.FolderRootHash}, which instead takes an entire top-level or second-level
folder as input. These two usages appear in Listings \ref{code:ack-shard} and
\ref{code:send-shard-server}, respectively.  

At first glance, one might assume that these calls are directly invoking the Rust Merkle 
tree library. However, as explained in Section \ref{sec:merkle-tree-library}, the 
Merkle tree was implemented as a library in Rust, not as a standalone binary. 
To make it usable from the Go-based system, a dedicated binary was developed in Rust, 
wrapping the library functionality and exposing simple command-line options for root hash 
generation and proof verification. The flow in illustrated in Figure
\ref{fig:go-rust-merkle}.


\begin{figure}[h]
    \centering
    \begin{tikzpicture}[
        node distance=2cm,
    ]

    \tikzstyle{box} = [rectangle, minimum width=3cm, minimum height=1cm,
                       text centered, text width=3cm, draw=black];
                       
    \definecolor{goblue}{RGB}{121,212,253}
    \definecolor{rustorange}{RGB}{247,76,0}

    % Nodes
    \node[box, fill=goblue!50] (goapp) {Go application};
    \node[box, fill=goblue!50, below of=goapp] (gomodule) {\texttt{merkletree} Go module};
    \node[box, fill=goblue!50, below of=gomodule] (exec) {\texttt{exec.Command}};
    \node[box, fill=rustorange!50, below of=exec] (rustbin) {Rust binary (\texttt{mt-rs-bin})};
    \node[box, fill=rustorange!50, below of=rustbin] (rustlib) {Rust Merkle tree library};

    % Arrows
    \draw[->, thick] (goapp) -- (gomodule) node[midway, right] {API call};
    \draw[->, thick] (gomodule) -- (exec) node[midway, right] {system call};
    \draw[->, thick] (exec) -- (rustbin) node[midway, right] {binary execution};
    \draw[->, thick] (rustbin) -- (rustlib) node[midway, right] {library call};

    % Return arrows
    \draw[<-, thick, dashed] (gomodule.west) -- ++(-4,0) -- ++(0,-4) -- (rustbin.west)
        node[midway, above, sloped] {root hash / proof};

    \end{tikzpicture}
    \caption{Interaction between Go and Rust components. The Go \texttt{merkletree} 
    module invokes the Rust binary via \texttt{exec.Command}, which delegates 
    computation to the Rust Merkle tree library.}
    \label{fig:go-rust-merkle}
\end{figure}

A partial implementation of this Rust binary is shown in Listing
\ref{code:partial-mt-rs-1}. Depending on the flag \texttt{--file}, the binary constructs a 
Merkle tree either from a set of files or from raw data provided as arguments. For instance, 
after compiling and installing the binary as \texttt{mt-rs-bin}, the command 
\texttt{mt-rs-bin --file ff} computes the Merkle root hash for the folder \texttt{ff}, 
which corresponds to the use case in Listing \ref{code:send-shard-server}. Alternatively, 
aggregating per-agent root hashes allows commands such as 
\texttt{mt-rs-bin hash1-ff hash2-ff hash3-ff} to compute the global root hash for folder 
\texttt{ff}, as required in Listing \ref{code:ack-shard}. In both cases, the binary 
produces only the Merkle root hash as a string, which the Go application captures and stores.  

The binary can be extended with additional modes, such as proof verification. 
In this case, passing a \texttt{--proof} flag triggers Merkle proof validation instead of 
tree creation, as shown in Listing \ref{code:partial-mt-rs-2}. The proof is always generated 
and verified from the first leaf of the input data. The output is 
a boolean value (\texttt{true} or \texttt{false}) indicating whether the proof is valid.  

For completeness, the Go module \texttt{merkletree} provides methods for 
proof verification and root computation; their prototypes are in Listing \ref{code:prototypes-merkletree-module}. 
These are thin wrappers that internally call the Rust binary via 
\texttt{exec.Command} and capture its output. This design avoids the need for 
complex Go-Rust bindings, while taking advantage of the high-performance Rust 
implementation of the Merkle tree library. In this way, the performance-critical 
logic remains in Rust, whereas the Go application retains a clean and simple 
interface for Merkle tree operations.  

\paragraph{Signal corruptions}
Finally, this paragraph illustrates how the leader signals data corruption for a folder
to the other agents in the Raft cluster. In Listing
\ref{code:command-struct-for-raft}, the reader has already seen the command code
\texttt{SignalCorruption}, which is used here as the operation code. The
associated action is defined by a folder name (top-level or second-level) and a
boolean \texttt{CorruptionStatus} flag.

When the Corruption Check process (Section \ref{sec:check-corruption}) is triggered,
the leader follows the sequence shown in Figure
\ref{fig:sequence-diagram-check-corruptions}, while also respecting the flow for
handling offline agents as illustrated in Figure
\ref{fig:flow-chart-check-corruption}.

A partial implementation of this flow is presented in Listing
\ref{code:check-corruption-algorithm}. The global maps \texttt{hashes1} and
\texttt{hashes2} store folder names as keys and per-agent hashes as values, and
can be used as a backup if some agents are offline. If the first retrieved
\texttt{data} for a top-level folder does not verify against its Merkle proof,
the algorithm iteratively checks each second-level folder nested within it. The
function \texttt{merkletree.IsPathCorrupted} builds a Merkle proof from the
provided \texttt{data} and compares the computed root hash with the reference
root. Internally, it invokes the \texttt{mt-rs-bin} binary, as explained in the
previous paragraph.

Consistency is ensured by Raft: the function \texttt{saveCorruptionState} updates
a global \texttt{corruptions} map that associates each folder (independently of
its level) with a boolean value. Since every update is persisted in the Raft
log, each node in the cluster can rebuild the map locally and remain synchronized
with the cluster state.

