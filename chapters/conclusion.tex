\chapter{Conclusion}

This thesis presented the design and implementation of a Merkle-tree-based integrity verification protocol for geo-distributed storage systems.
The proposed system integrates Merkle trees, the Raft consensus algorithm, and Reed-Solomon codes to provide a reliable corruption detection mechanism that remains functional even when some agents are temporarily offline.

The developed prototype demonstrated that it is possible to maintain a consistent and verifiable integrity state across distributed agents without relying on full-file scans or constant node availability.
Raft ensures that integrity metadata is synchronised cluster-wide, while Merkle trees allow hierarchical and efficient integrity checks at the folder or subfolder level.

The experimental results confirmed the correctness and resilience of the
approach under different scenarios and network conditions.
However, the tests also revealed a clear performance limitation: verification time increases as the number of agents grows, mainly due to consensus and communication overhead.
The system remains functionally correct but becomes slower in larger clusters, particularly in scenarios with many small files, where metadata synchronisation dominates the cost.

Despite this, the proposed architecture fulfills its primary objective: enabling fault-tolerant, verifiable, and consistent corruption detection in distributed environments.
Its design makes it extensible to new optimisations and deployment models.

\paragraph{Future Work and Considerations}

Several directions emerge for further research and practical refinement:

\begin{itemize}
    \item \textbf{Adaptive Merkle-tree deployment per customer.}
    Since corruption checks behave differently depending on filesystem
    organisation, the system could apply Merkle trees adaptively based on customer usage.
    Some customers store many small files, while others handle few but very large files.
    Maintaining a dedicated filesystem per customer (for each agent) would also improve security and isolation, avoiding shared-state interference between unrelated datasets.

    \item \textbf{Scalability to a large number of agents.}
    Current experiments were limited to small clusters.
    It remains to be tested how the system behaves with several dozens of agents.
    If corruption checks require hours to complete, the approach could become impractical at large scale; therefore, performance profiling for large clusters is essential.

    \item \textbf{Optimizing folder and hash map access.}
    The use of \texttt{TrieMap}, potentially in a concurrent implementation, could optimise lookup speed for subfolders in \texttt{hashes2} or similar structures, reducing blocking time during verification.

    \item \textbf{Reducing redundant root retrievals.}
    Some performance gain might be achieved by avoiding unnecessary \emph{get root} operations, retrieving roots randomly or selectively.
    However, this introduces the risk of inconsistency or false negatives, so the safest approach remains to always query the current Merkle root from all online agents.

    \item \textbf{Parallel verification of independent folders.}
    Each top-level folder is independent; therefore, Merkle root verification can be executed concurrently across top-level directories.
    This parallelism could significantly reduce total verification time without compromising correctness.

    \item \textbf{Deeper folder hierarchies.}
    The current two-level folder model simplifies testing but may limit scalability.
    Adding additional folder levels could further localise corruption checks and reduce recomputation overhead, potentially improving performance for large and complex filesystems.
\end{itemize}

In summary, this work shows that a Merkle-tree-based corruption detection system coordinated through Raft can provide strong correctness and resilience guarantees for geo-distributed storage.
Although performance decreases with larger clusters, the approach remains a robust foundation for future distributed integrity systems.
With targeted optimisations, this proposed integrity verification protocol, could evolve into a practical and efficient solution for production-scale environments such as Cubbit.
