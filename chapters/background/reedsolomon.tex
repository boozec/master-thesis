\section{Reed-Solomon} \label{sec:reed-solomon}

This section provides background on Reed-Solomon coding to give the reader a clearer picture of Cubbit's infrastructure and the context in which the proposed integrity verification mechanism was tested.

\subsection{Classical Reed-Solomon coding}

Reed-Solomon error correction \cite{reed1960polynomial} is one of the most widely used error correction codes, applied in digital communications, storage systems, and network protocols.

A Reed-Solomon code is defined over a finite field $F_q$, where $F$ denotes the finite field and $q$ is the size of its alphabet (typically a power of two, e.g. $q=2^8$ for byte-oriented operations).  

Classically, a Reed-Solomon code is parameterized by two values: the block length $n$, representing the total number of symbols in a codeword (data plus redundancy), and the message length $k$, representing the number of original data symbols, with $k < n \leq q$.

The encoder maps the $k$ data symbols $m_0, m_1, \ldots, m_{k-1}$ to a polynomial of degree at most $k-1$:
\begin{equation}
P(x) = m_0 + m_1x + m_2x^2 + \cdots + m_{k-1}x^{k-1}.
\end{equation}

This polynomial is evaluated at $n$ distinct points $x_1, x_2, \ldots, x_n$ in $F_q$, producing $n$ encoded symbols. The first $k$ symbols correspond to the original data, while the remaining $n-k$ symbols are redundancy.  

The key property of Reed-Solomon coding is that any subset of $k$ symbols from the $n$ encoded symbols is sufficient to reconstruct the original message using polynomial interpolation (e.g. Lagrange interpolation). This allows recovery even if some symbols are lost or corrupted.

\subsection{Cubbit's adaptation} \label{sec:reed-solomon-in-cubbit}

Cubbit employs Reed-Solomon coding to store files reliably across multiple nodes in its geo-distributed network. Unlike the classical definition, in Cubbit's implementation the total number of shards stored across the network is $n+k$, where $n$ denotes the number of data shards and $k$ the number of redundancy shards.

For example, with three nodes, a file could be split into three data shards,
distributed one per node. If the system is configured with $k=1$, only $n=2$
shards are required to reconstruct the original file. Thus, even if one node is offline, the user can still recover the file successfully.  
